{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage profond - TD n°2\n",
    "__________\n",
    "Architectures DNN classiques appliquées à la classification de chiffres avec MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données et problème\n",
    "\n",
    "On réutilise le dataset MNIST déjà téléchargé au TD précédent. Cette fois-ci, on va charger les données avec le module `Dataloader` de pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3081,))])\n",
    "\n",
    "my_data_path = './data'\n",
    "train_set = datasets.MNIST( my_data_path, train=True, transform=trans, download=True )\n",
    "test_set = datasets.MNIST( my_data_path, train=False, transform=trans, download=True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulter la [documentation PyTorch](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) à propos des modules `Dataset` et `Dataloader`.\n",
    "\n",
    "NB : en pratique, on pourra définir [son propre dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) pour un cas d'utilisation donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=TODO) #TO DO\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=TODO) #TO DO\n",
    "\n",
    "\n",
    "print('total training batch number: {}'.format(TODO))\n",
    "print('total testing batch number: {}'.format(TODO))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation des images en utilisant le chargement des données avec `Dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some images\n",
    "# for an alternative see https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
    "def imshow(tensor, title=None):\n",
    "    img = tensor.cpu().clone()\n",
    "    img = img.squeeze()\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.5)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(5):\n",
    "    imshow(train_set.TODO , title='MNIST example ({})'.format(train_set.TODO) )\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modélisation MLP\n",
    "\n",
    "On commence par définir les tailles d'entrée et de sortie du réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition des tailles d'entrée et de sortie\n",
    "DATA_SIZE = TODO\n",
    "NUM_CLASSES = TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Régression softmax__\n",
    "\n",
    "Implémentation d'une classe `RegSoftNet` pour apprendre un modèle de régression softmax (généralisation à >  2 classesde la régression logistique).\n",
    "\n",
    "Ici on utilise simplement 1 couche cachée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegSoftNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegSoftNet, self).__init__()\n",
    "        self.fc = TODO\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, DATA_SIZE) # .view() equivalent to .reshape() for numpy / passer de 28*28 à  (784,) /  -1 : \"give me a tensor that has these many columns and you compute the appropriate number of rows that is necessary to make this happen\".\n",
    "        x = TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegSoftNet()\n",
    "model.to(device) # pour faire passer le modèle sur GPU / CPU\n",
    "\n",
    "# optimization hyperparameters\n",
    "optimizer = TODO\n",
    "loss_fn = TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : Utilisation de model.train(), model.eval(), et with torch.no_grad()\n",
    "* model.train() active les couches de type \"dropout\" ou \"batchnorm\". __Par défaut, tous les modules sont initialisés avec `train = True`__.\n",
    "* model.eval() désactive la mise à jour des couches de type \"dropout\" ou \"batchnorm\".\n",
    "* with torch.no_grad() économise de la mémoire et du temps de calcul au moment de l'inférence, dans la mesure où cela désactive la stockage des valeurs intermédiaires dans le graphe de calcul. Conserver en mémoire ces valeurs intermédiaires est utile pour effectuer la rétropropagation, mais inutile à l'inférence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement et évaluation\n",
    "\n",
    "for epoch in range(10):\n",
    "    # training\n",
    "    model.train() \n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx %100 ==0:\n",
    "            print('epoch {} batch {} [{}/{}] training loss: {}'.format(epoch,batch_idx,batch_idx*len(x),\n",
    "                    len(train_loader.dataset),loss.item()))\n",
    "    # testing\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        confusion = torch.zeros(NUM_CLASSES,NUM_CLASSES)\n",
    "        for batch_idx, (x, target) in enumerate(test_loader):\n",
    "            x, target = x.to(device), target.to(device)\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, target)\n",
    "            # _, prediction = torch.max(out.data, 1)\n",
    "            prediction = out.argmax(dim=1, keepdim=True) # index of the max log-probability\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "            # since 'prediction' and 'target' may be on the GPU memory\n",
    "            # thus (i,j) are on the GPU as well. They must be transfered\n",
    "            # to the CPU, where 'confusion' has been allocated\n",
    "            for i,j in zip(prediction,target):\n",
    "                confusion[i.to(\"cpu\"),j.to(\"cpu\")] += 1\n",
    "    taux_classif = 100. * correct / len(test_loader.dataset)\n",
    "    print('Test Accuracy: {}/{} (tx {:.2f}%, err {:.2f}%)\\n'.format(correct,\n",
    "     len(test_loader.dataset), taux_classif, 100.-taux_classif))\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    print(confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Perceptron multi-couche__\n",
    "\n",
    "On utilise ici 2 couches cachées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HIDDEN_1 = 256\n",
    "NUM_HIDDEN_2 = 256\n",
    "\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = TODO\n",
    "        self.fc2 = TODO\n",
    "        self.fc3 = TODO\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, DATA_SIZE) # reshape the tensor \n",
    "        x = TODO\n",
    "        x = TODO\n",
    "        x = TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPNet()\n",
    "model.to(device) # pour faire passer le modèle sur GPU / CPU\n",
    "\n",
    "# redéfinir l'optimiseur ! \n",
    "optimizer = TODO\n",
    "loss_fn = TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nouveau, on entraine le modèle et on affiche au fur et à mesure des \"epochs\" la matrice de confusion sur les données d'évaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement et évaluation\n",
    "\n",
    "for epoch in range(10):\n",
    "    # training\n",
    "    model.train() \n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx %100 ==0:\n",
    "            print('epoch {} batch {} [{}/{}] training loss: {}'.format(epoch,batch_idx,batch_idx*len(x),\n",
    "                    len(train_loader.dataset),loss.item()))\n",
    "    # testing\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        confusion = torch.zeros(NUM_CLASSES,NUM_CLASSES)\n",
    "        for batch_idx, (x, target) in enumerate(test_loader):\n",
    "            x, target = x.to(device), target.to(device)\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, target)\n",
    "            # _, prediction = torch.max(out.data, 1)\n",
    "            prediction = out.argmax(dim=1, keepdim=True) # index of the max log-probability\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "            # since 'prediction' and 'target' may be on the GPU memory\n",
    "            # thus (i,j) are on the GPU as well. They must be transfered\n",
    "            # to the CPU, where 'confusion' has been allocated\n",
    "            for i,j in zip(prediction,target):\n",
    "                confusion[i.to(\"cpu\"),j.to(\"cpu\")] += 1\n",
    "    taux_classif = 100. * correct / len(test_loader.dataset)\n",
    "    print('Test Accuracy: {}/{} (tx {:.2f}%, err {:.2f}%)\\n'.format(correct,\n",
    "     len(test_loader.dataset), taux_classif, 100.-taux_classif))\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    print(confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modélisation CNN\n",
    "\n",
    "Consulter la [documentation PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#conv2d) de la class `Conv2D`.\n",
    "\n",
    "> class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "Taille d'entrée : (N,Cin,H,W) x (N,Cin​,H,W) \n",
    "\n",
    "Taille de sortie : (N,Cout,Hout,Wout) x (N,Cout​,Hout​,Wout​)\n",
    "\n",
    "avec : \n",
    "* N : batch size\n",
    "* Cin et Cout : nombre de filtres respectivement en entrée et sortie (channels)\n",
    "* H et W : height and width des filtres en entrée\n",
    "* Hout et Wout : height and width des filtres en sortie\n",
    "\n",
    "TODO : calcul des dimensions de sortie des couches de convolution et de pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation d'un CNN avec deux couches convolutives\n",
    "# v1 sans Dropout\n",
    "# v2 ajouter une couche de Dropout après les 2 couches de conv\n",
    "\n",
    "NUM_CONV_1= TODO\n",
    "NUM_CONV_2= TODO\n",
    "NUM_FC= TODO\n",
    "\n",
    "class CNNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNNet,self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(TODO,TODO,5,1) # kernel_size = 5\n",
    "        self.conv_2 = nn.Conv2d(TODO,TODO,5,1) # kernel_size = 5\n",
    "        self.fc_1 = nn.Linear(TODO_H, TODO)\n",
    "        self.fc_2 = nn.Linear(TODO,NUM_CLASSES)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = x.view(-1,TODO_H)\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = self.fc_2(x)\n",
    "        return x\n",
    "        # en utilisant loss = F.nll_loss(output, target) on peut faire\n",
    "        # return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : de manière équivalente, en utilisant `loss = F.nll_loss(output, target)` au lieu de `loss = CrossEntropyLoss()` on peut écrire :\n",
    "\n",
    "> return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNNet()\n",
    "model.to(device) # pour faire passer le modèle sur GPU / CPU\n",
    "\n",
    "# optimization hyperparameters\n",
    "optimizer = TODO\n",
    "loss_fn = TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement et évaluation\n",
    "\n",
    "for epoch in range(10):\n",
    "    # training\n",
    "    model.train() \n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx %100 ==0:\n",
    "            print('epoch {} batch {} [{}/{}] training loss: {}'.format(epoch,batch_idx,batch_idx*len(x),\n",
    "                    len(train_loader.dataset),loss.item()))\n",
    "    # testing\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        confusion = torch.zeros(NUM_CLASSES,NUM_CLASSES)\n",
    "        for batch_idx, (x, target) in enumerate(test_loader):\n",
    "            x, target = x.to(device), target.to(device)\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, target)\n",
    "            # _, prediction = torch.max(out.data, 1)\n",
    "            prediction = out.argmax(dim=1, keepdim=True) # index of the max log-probability\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "            # since 'prediction' and 'target' may be on the GPU memory\n",
    "            # thus (i,j) are on the GPU as well. They must be transfered\n",
    "            # to the CPU, where 'confusion' has been allocated\n",
    "            for i,j in zip(prediction,target):\n",
    "                confusion[i.to(\"cpu\"),j.to(\"cpu\")] += 1\n",
    "    taux_classif = 100. * correct / len(test_loader.dataset)\n",
    "    print('Test Accuracy: {}/{} (tx {:.2f}%, err {:.2f}%)\\n'.format(correct,\n",
    "     len(test_loader.dataset), taux_classif, 100.-taux_classif))\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calcul de la taille d'un modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametres + buffers (e.g. batch norm)\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\" \n",
    "    in megabites --> divide by 1024**2\n",
    "    \"\"\"\n",
    "    param_size = 0\n",
    "    for p in model.parameters() :\n",
    "        param_size += TODO\n",
    "\n",
    "    buffer_size = 0\n",
    "    for b in model.buffers() :\n",
    "        buffer_size += TODO\n",
    "\n",
    "    return (param_size + buffer_size) / 1024**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sauvegarde des poids du modèle__\n",
    "\n",
    "Il y a deux manières de sauvegarder un modèle suivant si on utilise `state_dict()` ou pas. Comparer avec la [documentation PyTorch](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
    "\n",
    "> A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor. Note that only layers with learnable parameters (convolutional layers, linear layers, etc.) and registered buffers (batchnorm’s running_mean) have entries in the model’s state_dict. Optimizer objects (torch.optim) also have a state_dict, which contains information about the optimizer’s state, as well as the hyperparameters used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde\n",
    "print(model)\n",
    "torch.save(model, 'my_cnn.pth')\n",
    "\n",
    "# chargement\n",
    "model1 = torch.load('./my_cnn.pth')\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de state_dict : on accède aux paramètres du modèle\n",
    "\n",
    "# sauvegarde\n",
    "print(model.state_dict().keys())\n",
    "torch.save(model.state_dict(), 'my_cnn_params.pth')\n",
    "\n",
    "# chargement : il faut définir le modèle puis charger les poids\n",
    "model2 = CNNNet()\n",
    "model2.load_state_dict(torch.load('my_cnn_params.pth'))\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualisation des feature maps (cartes d'activation)__\n",
    "\n",
    "Se référer au script `visualize_cnn_features.py`. Il utilise les poids du CNN, que l'on vient de sauvegarder. \n",
    "\n",
    "Autres références pour visualiser les cartes d'activation d'un CNN : [un réseau plus profond](https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/)  et [une autre méthode de visualisation](https://blbadger.github.io/feature-visualization.html) (en optimisant l'entrée de manière à maximiser la réponse d'un filtre donné)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèles LSTM et Bi-LSTM\n",
    "\n",
    "Se référer à la [documentation PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) pour le module `LSTM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LSTM__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "sequence_length = 28 # on voit une image comme une chaine de 28 mots\n",
    "input_size = 28 # chaque mot fait 28 caractères\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LSTM model\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self,in_size,hidden_size, nb_layer, nb_classes):\n",
    "        super(LSTMNet,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nb_layer = nb_layer\n",
    "        self.nb_classes = nb_classes\n",
    "        self.lstm = nn.LSTM(in_size,hidden_size,nb_layer,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size,nb_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # initial states\n",
    "        h0 = torch.zeros(self.nb_layer, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.nb_layer, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out,_ = self.lstm(x, (h0,c0))\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMNet(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# training\n",
    "total_step = len(train_loader)\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(img,lab) in enumerate(train_loader):\n",
    "        img = img.reshape(-1,sequence_length,input_size).to(device)\n",
    "        lab = lab.to(device)\n",
    "\n",
    "        outputs = model(img)\n",
    "        loss = loss_fn(outputs,lab)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f} ({:.2f} s)'\n",
    "            .format(epoch+1, num_epochs, i+1, total_step,\n",
    "            loss.item(), time.time()-start))\n",
    "\n",
    "# test\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for img, lab in test_loader:\n",
    "        img = img.reshape(-1,sequence_length,input_size).to(device)\n",
    "        lab = lab.to(device)\n",
    "        outputs = model(img)\n",
    "        _, pred = torch.max(outputs.data,1)\n",
    "        total += lab.size(0)\n",
    "        correct += (pred == lab).sum().item()\n",
    "\n",
    "    print('Test Accuracy: {}%'.format(100. * correct / total) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BiLSTM__\n",
    "\n",
    "On passe l'option \"bidirectional\" à \"True\" dans nn.LSTM, et on adapte les dimensions des tenseurs dans le réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005 # for BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define BiLSTM model\n",
    "class BiLSTMNet(nn.Module):\n",
    "    def __init__(self,in_size,hidden_size, nb_layer, nb_classes):\n",
    "        super(BiLSTMNet,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nb_layer = nb_layer\n",
    "        self.nb_classes = nb_classes\n",
    "        self.lstm = nn.LSTM(in_size,hidden_size,nb_layer,batch_first=True,bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2,nb_classes)  # 2 for bidirection\n",
    "\n",
    "    def forward(self,x):\n",
    "        # initial states\n",
    "        h0 = torch.zeros(self.nb_layer*2, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.nb_layer*2, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out,_ = self.lstm(x, (h0,c0))\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMNet(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "total_step = len(train_loader)\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(img,lab) in enumerate(train_loader):\n",
    "        img = img.reshape(-1,sequence_length,input_size).to(device)\n",
    "        lab = lab.to(device)\n",
    "\n",
    "        outputs = model(img)\n",
    "        loss = loss_fn(outputs,lab)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f} ({:.2f} s)'\n",
    "            .format(epoch+1, num_epochs, i+1, total_step,\n",
    "            loss.item(), time.time()-start))\n",
    "\n",
    "# test\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for img, lab in test_loader:\n",
    "        img = img.reshape(-1,sequence_length,input_size).to(device)\n",
    "        lab = lab.to(device)\n",
    "        outputs = model(img)\n",
    "        _, pred = torch.max(outputs.data,1)\n",
    "        total += lab.size(0)\n",
    "        correct += (pred == lab).sum().item()\n",
    "\n",
    "    print('Test Accuracy: {}%'.format(100. * correct / total) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
