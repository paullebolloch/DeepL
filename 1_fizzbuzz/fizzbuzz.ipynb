{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD n°1 Deep Learning - Exo 1 FizzBuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On modélise le pb commme un pb de classification multi-classe.\n",
    "\n",
    "Les sorties possibles 0 / 1 / 2 / 3 correspondent respectivement \n",
    "* au nombre lui-même, \n",
    "* à fizz pour les multiples de 3, \n",
    "* buzz pour les multiples de 5,\n",
    "* fizzbuzz pour les multiples de 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Modélisation sortie de la vérité terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz (i, prediction) :\n",
    "    return [str(i),\"fizz\",\"buzz\",\"fizzbuzz\"][prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de la vérité terrain : [number, fizz, buzz, fizzbuzz]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(i):\n",
    "    if i % 15 == 0:\n",
    "        return 3\n",
    "    elif i % 5 == 0:\n",
    "        return 2\n",
    "    elif i % 3 == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez vos fonctions en vérifiant que vous renvoyez la bonne sortie pour $i=1,\\dots,19$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 20): \n",
    "    print(\n",
    "        i, \n",
    "        fizz_buzz_encode(i),# classe  \n",
    "        fizz_buzz(i , fizz_buzz_encode(i)) # classe -> résultat i ? fizz ? buzz ? fizzbuzz ?\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modélisation des entrées et des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodage binaire \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DIGITS = 10\n",
    "def binary_encode (i , num_digits=NUM_DIGITS ) :\n",
    "    return [i >> d & 1 for d in range(num_digits)] #TO DO. Prend un nombre i et le convertit en binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "start_train, end_train = 101,2**NUM_DIGITS # 101, 1024\n",
    "start_test, end_test = 1, 101\n",
    "\n",
    "# train\n",
    "X_train = torch.FloatTensor ([ binary_encode (i , NUM_DIGITS ) for i in range (start_train, end_train)])\n",
    "Y_train = torch.LongTensor ([fizz_buzz_encode(i) for i in range(start_train, end_train)]).squeeze()\n",
    "\n",
    "# test\n",
    "X_test = torch.FloatTensor ([ binary_encode (i , NUM_DIGITS) for i in range (start_test, end_test) ])\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle, de la loss et de l'optimiseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre de neurones dans la couche cachée\n",
    "NUM_HIDDEN = 100\n",
    "\n",
    "# définition du MLP à 1 couche cachée (non linearite ReLU)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(NUM_DIGITS, NUM_HIDDEN), # Entrée -> 10 digits\n",
    "    torch.nn.ReLU(), #Non linéarité ReLU\n",
    "    torch.nn.Linear(NUM_HIDDEN, 4) # 4 classes -> sortie\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fonction de coût__ \n",
    "\n",
    "CrossEntropyLoss vs NLLLoss ? (negative log likelihood)\n",
    "\n",
    "CE loss : The input is expected to contain the unnormalized logits for each class, *which do not need to be positive or sum to 1, in general*. \n",
    "\n",
    "NLLLoss : il faut avoir calculé un softmax\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy formule générique que l'on utilise tout le temps. Elle dépend de la forme exacte de la sortie du réseau de neurones \n",
    "# Sigmoid pour problèmes binaires\n",
    "# Softmax pour problèmes multi-classes\n",
    "# On pourrait en soi avoir une sigmoid par classe, mais on préfère softmax comme on est exclusif\n",
    "# NLLL : Negative Log Likelihood Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction de coût\n",
    "loss_fn = torch.nn.CrossEntropyLoss()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimiseur --> choix du learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05) # Mettre à jour tous les paramètres du modèle\n",
    "# Hyperparamètre important à cette étape: le learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On doit toujours avoir un ensemble de train (paramètres), un ensemble de val (différents hyperparamètres - learning rate, nombre de couches, etc) \n",
    "# et un ensemble de test (pour la généralisation), on monitore si ça se passe bien sur val\n",
    "# on test une fois sur un ensemble de test (pour la généralisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 training loss 1.157\n",
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30'\n",
      " '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44'\n",
      " '45' '46' '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58'\n",
      " '59' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72'\n",
      " '73' '74' '75' '76' '77' '78' '79' '80' '81' '82' '83' '84' '85' '86'\n",
      " '87' '88' '89' '90' '91' '92' '93' '94' '95' '96' '97' '98' '99' '100']\n",
      "epoch 100 training loss 1.132\n",
      "epoch 200 training loss 1.119\n",
      "epoch 300 training loss 1.097\n",
      "epoch 400 training loss 1.062\n",
      "epoch 500 training loss 1.006\n",
      "epoch 600 training loss 0.932\n",
      "epoch 700 training loss 0.856\n",
      "epoch 800 training loss 0.779\n",
      "epoch 900 training loss 0.709\n",
      "epoch 1000 training loss 0.63\n",
      "['1' 'fizz' 'fizz' '4' '5' 'fizz' '7' '8' 'fizz' '10' '11' 'fizz' '13'\n",
      " '14' '15' '16' '17' 'fizz' '19' '20' '21' '22' '23' 'fizz' '25' '26'\n",
      " 'fizz' '28' '29' '30' '31' '32' 'fizz' '34' 'buzz' 'fizz' '37' '38'\n",
      " 'fizz' '40' '41' 'fizz' '43' '44' '45' '46' '47' 'fizz' '49' 'buzz'\n",
      " 'buzz' '52' '53' 'fizz' 'buzz' '56' '57' '58' '59' '60' '61' '62' '63'\n",
      " '64' '65' 'fizz' '67' '68' 'fizz' '70' '71' 'fizz' '73' '74' 'fizzbuzz'\n",
      " '76' '77' 'fizz' '79' 'buzz' '81' '82' '83' 'fizz' '85' '86' '87' '88'\n",
      " '89' '90' '91' 'fizz' 'fizz' '94' '95' 'fizz' '97' '98' 'fizz' 'fizz']\n",
      "epoch 1100 training loss 0.557\n",
      "epoch 1200 training loss 0.483\n",
      "epoch 1300 training loss 0.419\n",
      "epoch 1400 training loss 0.365\n",
      "epoch 1500 training loss 0.321\n",
      "epoch 1600 training loss 0.274\n",
      "epoch 1700 training loss 0.24\n",
      "epoch 1800 training loss 0.212\n",
      "epoch 1900 training loss 0.189\n",
      "epoch 2000 training loss 0.17\n",
      "['1' 'fizz' 'fizz' '4' '5' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' '20' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizzbuzz' '31' 'buzz' 'fizz' 'buzz' 'buzz' 'fizz'\n",
      " '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' 'buzz' 'fizz' '64' '65' 'fizz' '67' '68' 'fizz' '70' '71'\n",
      " 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82' '83'\n",
      " 'fizz' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' 'buzz' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "epoch 2100 training loss 0.152\n",
      "epoch 2200 training loss 0.138\n",
      "epoch 2300 training loss 0.126\n",
      "epoch 2400 training loss 0.116\n",
      "epoch 2500 training loss 0.106\n",
      "epoch 2600 training loss 0.098\n",
      "epoch 2700 training loss 0.091\n",
      "epoch 2800 training loss 0.084\n",
      "epoch 2900 training loss 0.078\n",
      "epoch 3000 training loss 0.073\n",
      "['1' 'fizz' 'fizz' '4' '5' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' 'buzz' 'buzz' 'fizz'\n",
      " '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' 'buzz' 'fizz' '64' '65' 'fizz' '67' '68' '69' 'buzz' '71'\n",
      " 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82' '83'\n",
      " 'fizz' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' 'buzz' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "epoch 3100 training loss 0.068\n",
      "epoch 3200 training loss 0.064\n",
      "epoch 3300 training loss 0.06\n",
      "epoch 3400 training loss 0.057\n",
      "epoch 3500 training loss 0.053\n",
      "epoch 3600 training loss 0.05\n",
      "epoch 3700 training loss 0.048\n",
      "epoch 3800 training loss 0.045\n",
      "epoch 3900 training loss 0.043\n",
      "epoch 4000 training loss 0.041\n",
      "['1' 'fizz' 'fizz' '4' '5' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' 'buzz' 'buzz' 'fizz'\n",
      " '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' '65' 'fizz' '67' '68' '69' 'buzz' '71'\n",
      " 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82' '83'\n",
      " 'fizz' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' 'buzz' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "epoch 4100 training loss 0.039\n",
      "epoch 4200 training loss 0.037\n",
      "epoch 4300 training loss 0.035\n",
      "epoch 4400 training loss 0.034\n",
      "epoch 4500 training loss 0.032\n",
      "epoch 4600 training loss 0.031\n",
      "epoch 4700 training loss 0.03\n",
      "epoch 4800 training loss 0.029\n",
      "epoch 4900 training loss 0.028\n",
      "epoch 5000 training loss 0.027\n",
      "['1' 'fizz' 'fizz' '4' '5' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' 'buzz' 'buzz' 'fizz'\n",
      " '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' '65' 'fizz' '67' '68' '69' 'buzz' '71'\n",
      " 'fizz' '73' '74' 'fizzbuzz' 'fizz' '77' 'fizz' '79' 'buzz' '81' '82' '83'\n",
      " 'fizz' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' 'buzz' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "epoch 5100 training loss 0.026\n",
      "epoch 5200 training loss 0.025\n",
      "epoch 5300 training loss 0.024\n",
      "epoch 5400 training loss 0.023\n",
      "epoch 5500 training loss 0.022\n",
      "epoch 5600 training loss 0.021\n",
      "epoch 5700 training loss 0.021\n",
      "epoch 5800 training loss 0.02\n",
      "epoch 5900 training loss 0.019\n",
      "epoch 6000 training loss 0.019\n",
      "['1' 'fizz' 'fizz' '4' '5' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' 'buzz' 'buzz' 'fizz'\n",
      " '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' 'fizz' '62' 'fizz' '64' '65' 'fizz' '67' '68' '69' 'buzz' '71'\n",
      " 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82' '83'\n",
      " 'fizz' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' 'buzz' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "epoch 6100 training loss 0.018\n",
      "epoch 6200 training loss 0.018\n",
      "epoch 6300 training loss 0.017\n",
      "epoch 6400 training loss 0.017\n",
      "epoch 6500 training loss 0.016\n",
      "epoch 6600 training loss 0.016\n",
      "epoch 6700 training loss 0.015\n",
      "epoch 6800 training loss 0.015\n",
      "epoch 6900 training loss 0.015\n",
      "epoch 7000 training loss 0.014\n",
      "['1' 'fizz' 'fizz' '4' '5' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' 'buzz' 'buzz' 'fizz'\n",
      " '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' 'fizz' '62' 'fizz' '64' '65' 'fizz' '67' '68' '69' 'buzz' '71'\n",
      " 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82' '83'\n",
      " 'fizz' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' 'buzz' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "epoch 7100 training loss 0.014\n",
      "epoch 7200 training loss 0.014\n",
      "epoch 7300 training loss 0.013\n",
      "epoch 7400 training loss 0.013\n",
      "epoch 7500 training loss 0.013\n",
      "epoch 7600 training loss 0.013\n",
      "epoch 7700 training loss 0.012\n",
      "epoch 7800 training loss 0.012\n",
      "epoch 7900 training loss 0.012\n",
      "epoch 8000 training loss 0.012\n",
      "['1' 'fizz' 'fizz' '4' '5' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' 'buzz' 'buzz' 'fizz'\n",
      " '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' 'fizz' '62' 'fizz' '64' '65' 'fizz' '67' '68' '69' 'buzz' '71'\n",
      " 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82' '83'\n",
      " 'fizz' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' 'buzz' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "epoch 8100 training loss 0.011\n",
      "epoch 8200 training loss 0.011\n",
      "epoch 8300 training loss 0.011\n",
      "epoch 8400 training loss 0.011\n",
      "epoch 8500 training loss 0.01\n",
      "epoch 8600 training loss 0.01\n",
      "epoch 8700 training loss 0.01\n",
      "epoch 8800 training loss 0.01\n",
      "epoch 8900 training loss 0.01\n",
      "epoch 9000 training loss 0.01\n",
      "['1' 'fizz' 'fizz' '4' '5' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' 'buzz' 'buzz' 'fizz'\n",
      " '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' 'fizz' '62' 'fizz' '64' '65' 'fizz' '67' '68' '69' 'buzz' '71'\n",
      " 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82' '83'\n",
      " 'fizz' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' 'buzz' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "epoch 9100 training loss 0.009\n",
      "epoch 9200 training loss 0.009\n",
      "epoch 9300 training loss 0.009\n",
      "epoch 9400 training loss 0.009\n",
      "epoch 9500 training loss 0.009\n",
      "epoch 9600 training loss 0.009\n",
      "epoch 9700 training loss 0.008\n",
      "epoch 9800 training loss 0.008\n",
      "epoch 9900 training loss 0.008\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "raw_data_test = np.arange(1, 101) # valeurs de test\n",
    "\n",
    "for epoch in range(10000):\n",
    "    for start in range(0, len(X_train), BATCH_SIZE):\n",
    "        end = start + BATCH_SIZE\n",
    "        batchX = X_train[start:end] # Pas de problème au bord de la liste, on peut dépasser la taille de la liste en slicing\n",
    "        batchY = Y_train[start:end]\n",
    "\n",
    "        # prediction et calcul de la loss\n",
    "        y_pred = model(batchX)\n",
    "        loss = loss_fn(y_pred, batchY)\n",
    "    \n",
    "        # mettre les gradients à 0 avant la passe retour (backward)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # rétro-propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # calcul coût  (et affichage)\n",
    "    loss = loss_fn( model(X_train), Y_train)\n",
    "    if epoch%100 == 0:\n",
    "        print('epoch {} training loss {}'.format(epoch, round(loss.item(), 3)))\n",
    "\n",
    "    # visualisation des résultats en cours d'apprentissage\n",
    "    if(epoch%1000==0):\n",
    "        Y_test_pred = model(X_test)\n",
    "        val, idx = torch.max(Y_test_pred,1)\n",
    "        ii=idx.data.numpy()\n",
    "        # numbers = np.arange(1, 101)\n",
    "        output = np.vectorize(fizz_buzz)(raw_data_test, ii)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Affichage des résultats__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Final result ============\n",
      "['1' 'fizz' 'fizz' '4' '5' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' '21' '22' '23' 'fizz' '25'\n",
      " '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' 'buzz' 'buzz' 'fizz'\n",
      " '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' 'fizz' '62' 'fizz' '64' '65' 'fizz' '67' '68' '69' '70' '71'\n",
      " 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82' '83'\n",
      " 'fizz' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' 'buzz' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "============== Final result ============\n",
      "['1', 'fizz', 'fizz', '4', '5', 'fizz', '7', '8', 'fizz', 'buzz', '11', 'fizz', '13', '14', 'fizzbuzz', '16', '17', 'fizz', '19', 'buzz', '21', '22', '23', 'fizz', '25', '26', 'fizz', '28', '29', 'fizzbuzz', '31', '32', 'fizz', 'buzz', 'buzz', 'fizz', '37', '38', 'fizz', 'buzz', '41', 'fizz', '43', '44', 'fizzbuzz', '46', '47', 'fizz', '49', 'buzz', 'fizz', '52', '53', 'fizz', 'buzz', '56', 'fizz', '58', '59', 'fizzbuzz', 'fizz', '62', 'fizz', '64', '65', 'fizz', '67', '68', '69', '70', '71', 'fizz', '73', '74', 'fizzbuzz', '76', '77', 'fizz', '79', 'buzz', '81', '82', '83', 'fizz', 'buzz', '86', '87', '88', '89', 'fizzbuzz', '91', 'buzz', 'fizz', '94', 'buzz', 'fizz', '97', '98', 'fizz', 'buzz']\n"
     ]
    }
   ],
   "source": [
    "# Sortie finale (calcul lisible)\n",
    "Y_test_pred = model(X_test)\n",
    "val, idx = torch.max(Y_test_pred,1)\n",
    "ii=idx.data.numpy()\n",
    "output = np.vectorize(fizz_buzz)(raw_data_test, ii)\n",
    "print(\"============== Final result ============\")\n",
    "print(output)\n",
    "\n",
    "# Sortie finale (calcul plus compact des predictions)\n",
    "Y_test_pred = model(X_test)\n",
    "predictions = zip(range(1, 101), list(Y_test_pred.max(1)[1].data.tolist()))\n",
    "print(\"============== Final result ============\")\n",
    "print ([fizz_buzz(i, x) for (i, x) in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calcul des performances (classification accuracy)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gtY \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([fizz_buzz_encode(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m raw_data_test])  \u001b[38;5;66;03m# ground truth\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest perf: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(gtY \u001b[38;5;241m==\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test_pred\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:1149\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "gtY = np.array([fizz_buzz_encode(i) for i in raw_data_test])  # ground truth\n",
    "print(\"test perf: \", np.mean(gtY == np.array(Y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Avec un validation set !__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VAL = 100\n",
    "p = np.random.permutation(range(len(X_train)))\n",
    "# permettre de mélanger les nombres\n",
    "X_train, Y_train = X_train[..., :], Y_train[...]\n",
    "X_val, Y_val = X_train[..., :], Y_train[...]\n",
    "X_train, Y_train = X_train[...:, :], Y_train[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_HIDDEN = 100\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(NUM_DIGITS, NUM_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(NUM_HIDDEN, 4)\n",
    "    )\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    for start in range(0, len(X_train), BATCH_SIZE):\n",
    "        end = start + BATCH_SIZE\n",
    "        batchX = X_train[start:end]\n",
    "        batchY = Y_train[start:end]\n",
    "\n",
    "        # prediction et calcul de la loss\n",
    "        y_pred = model(batchX)\n",
    "        loss = loss_fn(y_pred, batchY)\n",
    "    \n",
    "        # mettre les gradients à 0 avant la passe retour (backward)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # rétro-propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # calcul coût  (et affichage)\n",
    "    loss = loss_fn( model(X_train), Y_train)\n",
    "    if epoch%100 == 0:\n",
    "        print('epoch {} training loss {}'.format(epoch, round(loss.item(), 3)))\n",
    "\n",
    "    # visualisation des résultats en cours d'apprentissage\n",
    "    # cette fois-ci sur l'ensemble de validation\n",
    "    if(epoch%1000==0):\n",
    "        # train acc\n",
    "        Y_train_pred = model(X_train)\n",
    "        print(\"train perf: \", np.mean(Y_train.data.numpy() == Y_train_pred.max(1)[1].data.numpy() ) )\n",
    "        Y_val_pred = model(X_val)\n",
    "        print(\"val perf: \", np.mean(Y_val.data.numpy() == Y_val_pred.max(1)[1].data.numpy() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sortie finale (calcul lisible)\n",
    "Y_test_pred = model(X_test)\n",
    "val, idx = torch.max(Y_test_pred,1)\n",
    "ii=idx.data.numpy()\n",
    "output = np.vectorize(fizz_buzz)(raw_data_test, ii)\n",
    "print(\"============== Final result ============\")\n",
    "print(output)\n",
    "\n",
    "# Sortie finale (calcul plus compact des predictions)\n",
    "Y_test_pred = model(X_test)\n",
    "predictions = zip(range(1, 101), list(Y_test_pred.max(1)[1].data.tolist()))\n",
    "print(\"============== Final result ============\")\n",
    "print ([fizz_buzz(i, x) for (i, x) in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Experiment with different settings__\n",
    "\n",
    "* learning rate\n",
    "* optimizer\n",
    "* scheduler\n",
    "* number of training samples\n",
    "* architecture of the MLP (number of hidden units)\n",
    "* number of epochs\n",
    "* ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
