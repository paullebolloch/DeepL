{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage profond - TD n°3\n",
    "__________\n",
    "Transfert d'apprentissage (transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "__Modélisation__\n",
    "\n",
    "On considère un problème d'apprentissage de logos (6 logos de marques de bière, en environnement réel). \n",
    "Comment modéliser le problème si : \n",
    "- a. on suppose qu'il n'y a qu'un seul logo par image ? \n",
    "- b. si on veut pouvoir reconnaître la présence de plusieurs logos par image ? \n",
    "\n",
    "Réponse a. Softmax\n",
    "Réponse b. Multi sigmoïd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. __Préparation des données__\n",
    "\n",
    "__Chargement__\n",
    "\n",
    "Les données sont disponibles sur [GoogleDrive](https://drive.usercontent.google.com/download?id=1ec2n18lbI71c0IS7RoixzAe3D67nlEgE&export=download). \n",
    "\n",
    "Les images sont groupées par classes (un dossier = une classe). Cela nous permet d'utiliser la fonction `datasets.ImageFolder` de PyTorch afin de charger les données (cf TPs précédents utilisation d'un DataLoader). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/paullebolloch/Documents/3A_CS/ia/DeepL/TDDL/3_transfer'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corona', 'chimay', 'tsingtao', 'fosters', 'carlsberg', 'guiness']\n"
     ]
    }
   ],
   "source": [
    "# on lit une première fois les images du dataset\n",
    "# TODO adapter le path selon l'endroit où sont stockées les données\n",
    "image_directory = \"./data/td3_data\"\n",
    "\n",
    "print(os.listdir(image_directory))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Normalisation__\n",
    "\n",
    "Dans la suite, on va utiliser un modèle pré-entrainé sur le dataset ImageNet-1k (aussi appelé ILSVRC dataset, 1000 classes tirées de ImageNet-21k, 1.2 millions d'images). On applique aux données cibles une normalisation définie à partir des statistiques calculées sur le dataset source (moyenne et écarts types des valeurs des pixels, entre 0 et 1). On applique aussi un reformatage pour obtenir des images de 224 par 224 pixels.\n",
    "\n",
    "Dataset source = ImageNet-1k / \n",
    "Dataset cible = Beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420\n"
     ]
    }
   ],
   "source": [
    "# Normalisation des images pour les modèles pré-entraînés PyTorch\n",
    "# voir: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "# et ici pour les « explications » sur les valeurs exactes: https://github.com/pytorch/vision/issues/1439\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# première lecture des données\n",
    "dataset_full = datasets.ImageFolder(image_directory, data_transforms)\n",
    "print(len(dataset_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Motivation__\n",
    "\n",
    "On dispose seulement de 420 images réparties en 6 classes ! Pas suffisant pour entrainer un réseau de neurones profond de plusieurs millions de paramètres... D'où l'intérêt d'utiliser les poids d'un modèle déjà entrainé sur un autre dataset de plus grande taille. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes : ['carlsberg', 'chimay', 'corona', 'fosters', 'guiness', 'tsingtao']\n",
      "Mapping class to index : {'carlsberg': 0, 'chimay': 1, 'corona': 2, 'fosters': 3, 'guiness': 4, 'tsingtao': 5}\n",
      "('./data/td3_data/carlsberg/1179199291.jpg', 0)\n"
     ]
    }
   ],
   "source": [
    "# some useful info\n",
    "print(\"Classes :\", dataset_full.classes)\n",
    "print(\"Mapping class to index :\", dataset_full.class_to_idx)\n",
    "print(dataset_full.samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz\n",
    "for ii in range(3) : \n",
    "    img_path, label = dataset_full.samples[ii]\n",
    "    with Image.open(img_path) as img:    \n",
    "        img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Partage des données__ \n",
    "\n",
    "Pour cela, utiliser la fonction [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) de scikit-learn) avec les proportions suivantes :\n",
    "- train = 60 % \n",
    "- val = 15 %\n",
    "- test = 25 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images de train : 252\n",
      "Nombre d'images de val : 63\n",
      "Nombre d'images de test : 105\n"
     ]
    }
   ],
   "source": [
    "# on split en train, val et test à partir de la liste complète\n",
    "np.random.seed(42)\n",
    "samples_train, samples_test = train_test_split(dataset_full.samples, test_size=0.25) # train+val vs test\n",
    "samples_train, samples_val = train_test_split(samples_train, test_size=0.2) # train vs val, 15/75 = 0.2\n",
    "\n",
    "print(\"Nombre d'images de train : %i\" % len(samples_train))\n",
    "print(\"Nombre d'images de val : %i\" % len(samples_val))\n",
    "print(\"Nombre d'images de test : %i\" % len(samples_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit un `DataLoader` pour chacun des sous-ensembles de données. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on définit les datasets et loaders pytorch à partir des listes d'images de train / val / test\n",
    "\n",
    "dataset_train = datasets.ImageFolder(image_directory, data_transforms)\n",
    "dataset_train.samples = samples_train\n",
    "dataset_train.imgs = samples_train\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=32, \n",
    "                                           shuffle=True, num_workers=4) #dataloader sert à charger les données en batch\n",
    "\n",
    "dataset_val = datasets.ImageFolder(image_directory, data_transforms)\n",
    "dataset_val.samples = samples_val\n",
    "dataset_val.imgs = samples_val\n",
    "\n",
    "dataset_test = datasets.ImageFolder(image_directory, data_transforms)\n",
    "dataset_test.samples = samples_test\n",
    "dataset_test.imgs = samples_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification : toutes les classes doivent être représentées dans le jeu de données d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apprentissage sur 6 classes\n"
     ]
    }
   ],
   "source": [
    "# détermination du nombre de classes (nb_classes=6)\n",
    "# vérification que les labels sont bien dans [0, nb_classes]\n",
    "labels=[x[1] for x in samples_train]\n",
    "if np.min(labels) != 0:\n",
    "    print(\"Error: labels should start at 0 (min is %i)\" % np.min(labels))\n",
    "    sys.exit(-1)\n",
    "if np.max(labels) != (len(np.unique(labels))-1):\n",
    "    print(\"Error: labels should go from 0 to Nclasses (max label = {}; Nclasse = {})\".format(np.max(labels),len(np.unique(labels)))  )\n",
    "    sys.exit(-1)\n",
    "nb_classes = np.max(labels)+1\n",
    "# nb_classes = len(dataset_train.classes)\n",
    "print(\"Apprentissage sur {} classes\".format(nb_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reproductibilité et sources d'aléatoire*\n",
    "\n",
    "A votre avis, où se situent les sources d'aléatoire lorsque vous entrainez un réseau de neurones avec un framework d'apprentissage profond (PyTorch / TensorFlow) ? Y-a-t-il des sources d'aléatoire à l'inférence ? \n",
    "\n",
    "Liens utiles : [documentation pytorch](https://pytorch.org/docs/stable/notes/randomness.html), [un exemple chez Weight&Biases](https://wandb.ai/sauravmaheshkar/RSNA-MICCAI/reports/How-to-Set-Random-Seeds-in-PyTorch-and-Tensorflow--VmlldzoxMDA2MDQy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x109ca7090>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chargement d'un modèle pré-entrainé__\n",
    "\n",
    "Ici on utilise un réseau convolutif de type ResNet18, pré-entrainé sur ImageNet-1k. \n",
    "NB : jeter un oeil aux [modèles disponibles via pytorch](https://pytorch.org/vision/stable/models.html). Pour les architecures à base de transformers, de nombreux modèles sont aussi disponibles via le hub et les librairies [huggingface](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération du ResNet-18 pré-entraîné...\n"
     ]
    }
   ],
   "source": [
    "# Récupérer un réseau pré-entraîné (resnet-18)\n",
    "print(\"Récupération du ResNet-18 pré-entraîné...\")\n",
    "my_net = models.resnet18(weights='ResNet18_Weights.IMAGENET1K_V1') \n",
    "\n",
    "# The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
    "# The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. \n",
    "# You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# architecture\n",
    "print(my_net) \n",
    "\n",
    "#bn1: batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle est la taille de sortie de l'extracteur de features ?\n",
    "\n",
    "Réponse: 1000 classes d'imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transfert d'apprentissage\n",
    "\n",
    "Dans le cadre du transfert d'apprentissage, *on n'optimise pas les poids du réseau pr-entrainé sur nos données cibles*. On remplace simplement la couche de classification du réseau pré-entrainé par une nouvelle couche de classification, avec une taille adaptée au nombre de classes de notre problème. \n",
    "\n",
    "Pour apprendre à classer les images du dataset cible, on fige les poids du réseau pré-entrainé (partie \"extraction de caractéristiques\" / *feature extractor*) et on optimise les poids de la nouvelle couche de classification (partie \"décision\", une couche linéaire ici)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on indique qu'il est inutile de calculer les gradients des paramètres du réseau\n",
    "# (ils ne seront pas mis à jour) -> on fige la mise à jour des paramètres pour toutes les couches\n",
    "for param in my_net.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on remplace la dernière couche fully connected à 1000 sorties (classes d'ImageNet) par une fully connected à 6 sorties (nos classes).\n",
    "# par défaut, les gradients des paramètres cette couche seront bien calculés\n",
    "#  NB: par défaut, la couche réinitialisée a .requires_grad=True\n",
    "my_net.fc = nn.Linear(in_features=512, out_features=6, bias=True)\n",
    "# on pourrait aussi réinitaliser d'autres couches e.g. my_net.layer4[1].conv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# on utilisera le GPU (beaucoup plus rapide) si disponible, sinon on utilisera le CPU\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Vérifiez si 'mps' est disponible, sinon utilisez le CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "my_net.to(device)  # Utiliser le GPU (via MPS) ou le CPU en fonction de ce qui est disponible\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#device = torch.device(\"cpu\") # forcer en CPU s'il y a des problèmes de mémoire GPU (+ être patient...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Entrainement et évaluation__\n",
    "\n",
    "On donne une fonction d'entrainement et une fonction d'évaluation (cf TPs précédents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on définit une fonction d'évaluation\n",
    "def evaluate(model, dataset, criterion):\n",
    "    avg_loss = 0.\n",
    "    avg_accuracy = 0\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        n_correct = torch.sum(preds == labels)\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        avg_accuracy += n_correct\n",
    "        \n",
    "    return avg_loss / len(dataset), float(avg_accuracy) / len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction classique d'entraînement d'un modèle, voir TDs précédents\n",
    "PRINT_LOSS = True\n",
    "def train_model(model, loader_train, data_val, optimizer, criterion, n_epochs=10):\n",
    "    for epoch in range(n_epochs): # à chaque epochs\n",
    "        print(\"EPOCH % i\" % epoch)\n",
    "        for i, data in enumerate(loader_train): # itère sur les minibatchs via le loader apprentissage\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # on passe les données sur CPU / GPU\n",
    "            optimizer.zero_grad() # on réinitialise les gradients\n",
    "            outputs = model(inputs) # on calcule l'output\n",
    "            \n",
    "            loss = criterion(outputs, labels) # on calcule la loss\n",
    "            if PRINT_LOSS:\n",
    "                model.train(False)\n",
    "                loss_val, accuracy = evaluate(my_net, data_val, criterion)\n",
    "                model.train(True)\n",
    "                print(\"{} loss train: {:1.4f}\\t val {:1.4f}\\tAcc (val): {:.1%}\".format(i, loss.item(), loss_val, accuracy   ))\n",
    "            \n",
    "            loss.backward() # on effectue la backprop pour calculer les gradients\n",
    "            optimizer.step() # on update les gradients en fonction des paramètres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit une fonction de coût et un optimiseur. On utilise un faible taux d'apprentissage (learning rate fixé à 0.001) car on n'a besoin que d'optimiser la dernière couche du réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # fonction de coût\n",
    "optimizer = optim.SGD(my_net.fc.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apprentissage en transfer learning\n",
      "EPOCH  0\n",
      "0 loss train: 1.8752\t val 0.1204\tAcc (val): 19.0%\n",
      "1 loss train: 1.9514\t val 0.1199\tAcc (val): 19.0%\n",
      "2 loss train: 1.8581\t val 0.1185\tAcc (val): 20.6%\n",
      "3 loss train: 1.8688\t val 0.1173\tAcc (val): 19.0%\n",
      "4 loss train: 1.8164\t val 0.1166\tAcc (val): 15.9%\n",
      "5 loss train: 1.8525\t val 0.1160\tAcc (val): 15.9%\n",
      "6 loss train: 1.7824\t val 0.1155\tAcc (val): 19.0%\n",
      "7 loss train: 1.8062\t val 0.1149\tAcc (val): 22.2%\n",
      "EPOCH  1\n",
      "0 loss train: 1.7297\t val 0.1144\tAcc (val): 22.2%\n",
      "1 loss train: 1.7597\t val 0.1143\tAcc (val): 19.0%\n",
      "2 loss train: 1.8095\t val 0.1142\tAcc (val): 22.2%\n",
      "3 loss train: 1.7862\t val 0.1139\tAcc (val): 27.0%\n",
      "4 loss train: 1.5968\t val 0.1127\tAcc (val): 30.2%\n",
      "5 loss train: 1.6470\t val 0.1116\tAcc (val): 28.6%\n",
      "6 loss train: 1.6300\t val 0.1100\tAcc (val): 31.7%\n",
      "7 loss train: 1.7242\t val 0.1086\tAcc (val): 31.7%\n",
      "EPOCH  2\n",
      "0 loss train: 1.6231\t val 0.1069\tAcc (val): 36.5%\n",
      "1 loss train: 1.5243\t val 0.1049\tAcc (val): 39.7%\n",
      "2 loss train: 1.6038\t val 0.1035\tAcc (val): 42.9%\n",
      "3 loss train: 1.5408\t val 0.1019\tAcc (val): 44.4%\n",
      "4 loss train: 1.5057\t val 0.1006\tAcc (val): 44.4%\n",
      "5 loss train: 1.4307\t val 0.0994\tAcc (val): 46.0%\n",
      "6 loss train: 1.5016\t val 0.0983\tAcc (val): 46.0%\n",
      "7 loss train: 1.4762\t val 0.0970\tAcc (val): 46.0%\n",
      "EPOCH  3\n",
      "0 loss train: 1.4541\t val 0.0954\tAcc (val): 46.0%\n",
      "1 loss train: 1.3311\t val 0.0942\tAcc (val): 47.6%\n",
      "2 loss train: 1.3165\t val 0.0929\tAcc (val): 49.2%\n",
      "3 loss train: 1.4288\t val 0.0915\tAcc (val): 52.4%\n",
      "4 loss train: 1.3149\t val 0.0902\tAcc (val): 54.0%\n",
      "5 loss train: 1.3901\t val 0.0893\tAcc (val): 50.8%\n",
      "6 loss train: 1.3419\t val 0.0885\tAcc (val): 54.0%\n",
      "7 loss train: 1.1758\t val 0.0876\tAcc (val): 54.0%\n",
      "EPOCH  4\n",
      "0 loss train: 1.3030\t val 0.0866\tAcc (val): 57.1%\n",
      "1 loss train: 1.1381\t val 0.0860\tAcc (val): 60.3%\n",
      "2 loss train: 1.1593\t val 0.0853\tAcc (val): 60.3%\n",
      "3 loss train: 1.2251\t val 0.0848\tAcc (val): 61.9%\n",
      "4 loss train: 1.2817\t val 0.0845\tAcc (val): 60.3%\n",
      "5 loss train: 1.2528\t val 0.0841\tAcc (val): 58.7%\n",
      "6 loss train: 1.2337\t val 0.0835\tAcc (val): 58.7%\n",
      "7 loss train: 1.0219\t val 0.0825\tAcc (val): 58.7%\n",
      "EPOCH  5\n",
      "0 loss train: 1.1462\t val 0.0814\tAcc (val): 55.6%\n",
      "1 loss train: 1.1482\t val 0.0808\tAcc (val): 55.6%\n",
      "2 loss train: 1.1291\t val 0.0807\tAcc (val): 52.4%\n",
      "3 loss train: 1.0814\t val 0.0798\tAcc (val): 57.1%\n",
      "4 loss train: 1.1324\t val 0.0788\tAcc (val): 57.1%\n",
      "5 loss train: 1.0360\t val 0.0777\tAcc (val): 57.1%\n",
      "6 loss train: 1.0694\t val 0.0767\tAcc (val): 60.3%\n",
      "7 loss train: 1.1760\t val 0.0757\tAcc (val): 58.7%\n",
      "EPOCH  6\n",
      "0 loss train: 0.8531\t val 0.0749\tAcc (val): 60.3%\n",
      "1 loss train: 1.1871\t val 0.0738\tAcc (val): 61.9%\n",
      "2 loss train: 0.9255\t val 0.0726\tAcc (val): 61.9%\n",
      "3 loss train: 0.9852\t val 0.0724\tAcc (val): 61.9%\n",
      "4 loss train: 0.9959\t val 0.0725\tAcc (val): 60.3%\n",
      "5 loss train: 1.0164\t val 0.0719\tAcc (val): 61.9%\n",
      "6 loss train: 1.0090\t val 0.0718\tAcc (val): 63.5%\n",
      "7 loss train: 0.9345\t val 0.0709\tAcc (val): 65.1%\n",
      "EPOCH  7\n",
      "0 loss train: 0.9055\t val 0.0705\tAcc (val): 65.1%\n",
      "1 loss train: 0.8032\t val 0.0702\tAcc (val): 65.1%\n",
      "2 loss train: 0.7823\t val 0.0695\tAcc (val): 65.1%\n",
      "3 loss train: 1.0115\t val 0.0687\tAcc (val): 68.3%\n",
      "4 loss train: 0.8925\t val 0.0686\tAcc (val): 65.1%\n",
      "5 loss train: 0.9001\t val 0.0685\tAcc (val): 63.5%\n",
      "6 loss train: 0.7946\t val 0.0681\tAcc (val): 63.5%\n",
      "7 loss train: 0.9622\t val 0.0678\tAcc (val): 63.5%\n",
      "EPOCH  8\n",
      "0 loss train: 0.7667\t val 0.0670\tAcc (val): 65.1%\n",
      "1 loss train: 0.8906\t val 0.0665\tAcc (val): 65.1%\n",
      "2 loss train: 0.8753\t val 0.0666\tAcc (val): 66.7%\n",
      "3 loss train: 0.8296\t val 0.0662\tAcc (val): 66.7%\n",
      "4 loss train: 0.7552\t val 0.0656\tAcc (val): 66.7%\n",
      "5 loss train: 0.8493\t val 0.0651\tAcc (val): 65.1%\n",
      "6 loss train: 0.8468\t val 0.0650\tAcc (val): 63.5%\n",
      "7 loss train: 0.8002\t val 0.0645\tAcc (val): 66.7%\n",
      "EPOCH  9\n",
      "0 loss train: 0.8815\t val 0.0637\tAcc (val): 66.7%\n",
      "1 loss train: 0.7582\t val 0.0626\tAcc (val): 65.1%\n",
      "2 loss train: 0.7382\t val 0.0616\tAcc (val): 68.3%\n",
      "3 loss train: 0.8162\t val 0.0611\tAcc (val): 68.3%\n",
      "4 loss train: 0.7470\t val 0.0609\tAcc (val): 68.3%\n",
      "5 loss train: 0.6517\t val 0.0605\tAcc (val): 68.3%\n",
      "6 loss train: 0.7231\t val 0.0602\tAcc (val): 68.3%\n",
      "7 loss train: 0.6989\t val 0.0599\tAcc (val): 68.3%\n"
     ]
    }
   ],
   "source": [
    "my_net.train(True) # NB : pas indispensable ici comme on a fixé la partie extraction de features, \n",
    "# mais bonne pratique de façon générale\n",
    "# permet notamment d'activer / désactiver le dropout selon qu'on entraîne ou teste le modèle\n",
    "print(\"\\nApprentissage en transfer learning\")\n",
    "\n",
    "train_model(my_net, loader_train, dataset_val, optimizer, criterion, n_epochs=10)\n",
    "\n",
    "# une perte par minibath -> plusieurs pertes par epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (test): 74.3%\n"
     ]
    }
   ],
   "source": [
    "# évaluation\n",
    "my_net.train(False)\n",
    "loss, accuracy = evaluate(my_net, dataset_test, criterion)\n",
    "print(\"Accuracy (test): %.1f%%\" % (100 * accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adaptation fine des poids du réseau (*fine-tuning*)\n",
    "\n",
    "On réinitialise le réseau. Cette fois-ci, on va en utiliser les images de notre dataset cible pour mettre à jour (en totalité ou en partie) les paramètres du modèle dans les couches antérieures à la couche de décision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TODO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# on réinitialise le modèle resnet\u001b[39;00m\n\u001b[1;32m      2\u001b[0m my_net \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mresnet18(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResNet18_Weights.IMAGENET1K_V1\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m----> 4\u001b[0m my_net\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39m\u001b[43mTODO\u001b[49m, out_features\u001b[38;5;241m=\u001b[39mTODO, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m my_net\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# cette fois on veut mettre à jour tous les paramètres\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TODO' is not defined"
     ]
    }
   ],
   "source": [
    "# on réinitialise le modèle resnet\n",
    "my_net = models.resnet18(weights='ResNet18_Weights.IMAGENET1K_V1') \n",
    "\n",
    "my_net.fc = nn.Linear(in_features=TODO, out_features=TODO, bias=True)\n",
    "my_net.to(device)\n",
    "\n",
    "# cette fois on veut mettre à jour tous les paramètres\n",
    "params_to_update = TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque :  il est possible de ne sélectionner que quelques couches (plutôt parmi les \"dernières\", les plus proches de la couche de classificaiton et du calcul de la fonction de coût)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ici on restreint les couches dont on veut mettre à jour les paramètres\n",
    "\n",
    "list_of_layers_to_finetune=['fc.weight','fc.bias','layer4.1.conv2.weight','layer4.1.bn2. bias','layer4.1.bn2.weight']\n",
    "\n",
    "params_to_update=[]\n",
    "for name,param in my_net.named_parameters():\n",
    "    if name in list_of_layers_to_finetune:\n",
    "        print(\"fine tune \",name)\n",
    "        params_to_update.append(param)\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# sanity check \n",
    "print(\"Couches mises à jour :\")\n",
    "for name, param in my_net.named_parameters() : \n",
    "    if param.requires_grad :\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise un taux d'apprentissage relativement bas, on ne veut pas modifier brutalement les poids du réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la fonction de coût et de l'optimiseur \n",
    "criterion = TODO\n",
    "optimizer = optim.SGD(TODO, lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on ré-entraîne\n",
    "print(\"Apprentissage avec fine-tuning\")\n",
    "my_net.train(True)\n",
    "torch.manual_seed(42)\n",
    "train_model(my_net, loader_train, dataset_val, optimizer, criterion, n_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on ré-évalue les performances\n",
    "my_net.train(False)\n",
    "loss, accuracy = evaluate(my_net, dataset_test, criterion)\n",
    "print(\"Accuracy (test): %.1f%%\" % (100 * accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Autre architecture \n",
    "\n",
    "On a utilisé un réseau de type ResNet18 avec 10M de paramètres. Ici on se propose d'utiliser une architecture plus compacte : MobileNetv2 (), qui comporte 2.3M de paramètres. Comparer les architectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on définit un réseau avec une nouvelle architecture\n",
    "my_net = models.mobilenet_v2(weights='MobileNet_V2_Weights.IMAGENET1K_V1')\n",
    "print(my_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in my_net.parameters()))\n",
    "print(sum(p.numel() for p in my_net.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On note que cette architecture ne comporte pas de module `fc` accessible directement comme pour ResNet18 dans la partie précédente. La structure est `features` puis `classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remplacement de la couche de classification\n",
    "my_net.classifier[1] = nn.Linear(in_features=my_net.classifier[1].in_features, out_features=nb_classes, bias=True)\n",
    "my_net.to(device)\n",
    "\n",
    "# mise à jour de tous les paramètres\n",
    "params_to_update = my_net.parameters()\n",
    "\n",
    "# définition de la fonction de coût et de l'optimiseur \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# entrainement\n",
    "print(\"Apprentissage avec fine-tuning\\n\")\n",
    "my_net.train(True)\n",
    "torch.manual_seed(42)\n",
    "train_model(my_net, loader_train, dataset_val, optimizer, criterion, n_epochs=10)\n",
    "\n",
    "# évaluation des performances\n",
    "my_net.train(False)\n",
    "loss, accuracy = evaluate(my_net, dataset_test, criterion)\n",
    "print(\"\\nAccuracy (test): %.1f%%\" % (100 * accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Bonus : modélisation multi-labels\n",
    "\n",
    "En conservant l’hypothèse de classes exclusive (qui est fausse en pratique mais facilite l’annotation) il est néanmoins possible d’apprendre un modèle multi-labels, où chaque classe est reconnue indépendamment. C’est ce qui est proposé dans le programme `transfer_learning_pytorch_multilabel.py`.\n",
    "\n",
    "Points d'attentions : \n",
    "- ici on ne refait pas la labelisation des données, mais on modifie la manière d'entrainer le réseau \n",
    "\n",
    "- définition de la fonction de coût \n",
    "> criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "- seuil par défaut pour attribuer un label : 0.5\n",
    "\n",
    "- possibilité de déterminer un seuil de décision pour chacune des classes en se basant sur le dataset de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on redéfinit la fonction d'évaluation\n",
    "# one-hot encoding des labels pour calculer la BCELoss\n",
    "# pour l'accuracy, comme on ne dispose pas de la vérité terrain pour le cas multilabel, \n",
    "# on se rapporte au cas précédent avec un seul label. \n",
    " \n",
    "def evaluate(model, dataset):\n",
    "    avg_loss = 0.\n",
    "    avg_accuracy = 0\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        oh_labels = torch.nn.functional.one_hot(labels) \n",
    "        oh_labels = oh_labels.type(torch.FloatTensor)\n",
    "        inputs, oh_labels = inputs.to(device), oh_labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, oh_labels)\n",
    "        _, preds = torch.max(outputs, 1) \n",
    "        n_correct = torch.sum(preds.to(\"cpu\") == labels)\n",
    "        # autre méthode\n",
    "        # pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        # gt = oh_labels.argmax(dim=1, keepdim=True)\n",
    "        # n_correct = pred.eq(gt.view_as(pred)).sum().item()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        avg_accuracy += n_correct\n",
    "        \n",
    "    return avg_loss / len(dataset), float(avg_accuracy) / len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple avec mobilenet v2\n",
    "my_net = models.mobilenet_v2(weights='MobileNet_V2_Weights.IMAGENET1K_V1')\n",
    "my_net.classifier[1] = nn.Linear(in_features=my_net.classifier[1].in_features, out_features=nb_classes, bias=True)\n",
    "my_net.to(device)\n",
    "\n",
    "# mise à jour de tous les paramètres\n",
    "params_to_update = my_net.parameters()\n",
    "\n",
    "# définition de la fonction de coût et de l'optimiseur \n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# entrainement\n",
    "print(\"Apprentissage avec fine-tuning\")\n",
    "my_net.train(True)\n",
    "torch.manual_seed(42)\n",
    "train_model(my_net, loader_train, dataset_val, optimizer, criterion, n_epochs=10)\n",
    "\n",
    "# évaluation des performances\n",
    "my_net.train(False)\n",
    "loss, accuracy = evaluate(my_net, dataset_test, criterion)\n",
    "print(\"Accuracy (test): %.1f%%\" % (100 * accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour aller plus loin : quel pourcentage d'images comporte au moins 2 logos de marques différentes selon le modèle ? comment choisir des seuils de décision adaptés à chacune des classes ? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
